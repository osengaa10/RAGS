{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6d018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import together\n",
    "\n",
    "import logging\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "from pydantic import Extra, Field, root_validator, model_validator\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e51fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 models available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Austism/chronos-hermes-13b',\n",
       " 'EleutherAI/llemma_7b',\n",
       " 'EleutherAI/pythia-12b-v0',\n",
       " 'EleutherAI/pythia-1b-v0',\n",
       " 'EleutherAI/pythia-2.8b-v0',\n",
       " 'EleutherAI/pythia-6.9b',\n",
       " 'Gryphe/MythoMax-L2-13b',\n",
       " 'HuggingFaceH4/starchat-alpha',\n",
       " 'NousResearch/Nous-Hermes-13b',\n",
       " 'NousResearch/Nous-Hermes-Llama2-13b',\n",
       " 'NousResearch/Nous-Hermes-Llama2-70b',\n",
       " 'NousResearch/Nous-Hermes-llama-2-7b',\n",
       " 'NumbersStation/nsql-llama-2-7B',\n",
       " 'Open-Orca/Mistral-7B-OpenOrca',\n",
       " 'OpenAssistant/llama2-70b-oasst-sft-v10',\n",
       " 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
       " 'OpenAssistant/stablelm-7b-sft-v7-epoch-3',\n",
       " 'Phind/Phind-CodeLlama-34B-Python-v1',\n",
       " 'Phind/Phind-CodeLlama-34B-v2',\n",
       " 'SG161222/Realistic_Vision_V3.0_VAE',\n",
       " 'WizardLM/WizardCoder-15B-V1.0',\n",
       " 'WizardLM/WizardCoder-Python-34B-V1.0',\n",
       " 'WizardLM/WizardLM-70B-V1.0',\n",
       " 'bigcode/starcoder',\n",
       " 'databricks/dolly-v2-12b',\n",
       " 'databricks/dolly-v2-3b',\n",
       " 'databricks/dolly-v2-7b',\n",
       " 'defog/sqlcoder',\n",
       " 'garage-bAInd/Platypus2-70B-instruct',\n",
       " 'huggyllama/llama-13b',\n",
       " 'huggyllama/llama-30b',\n",
       " 'huggyllama/llama-65b',\n",
       " 'huggyllama/llama-7b',\n",
       " 'lmsys/fastchat-t5-3b-v1.0',\n",
       " 'lmsys/vicuna-13b-v1.5-16k',\n",
       " 'lmsys/vicuna-13b-v1.5',\n",
       " 'lmsys/vicuna-7b-v1.5',\n",
       " 'mistralai/Mistral-7B-Instruct-v0.1',\n",
       " 'mistralai/Mistral-7B-v0.1',\n",
       " 'prompthero/openjourney',\n",
       " 'runwayml/stable-diffusion-v1-5',\n",
       " 'stabilityai/stable-diffusion-2-1',\n",
       " 'stabilityai/stable-diffusion-xl-base-1.0',\n",
       " 'teknium/OpenHermes-2-Mistral-7B',\n",
       " 'togethercomputer/CodeLlama-13b-Instruct',\n",
       " 'togethercomputer/CodeLlama-13b-Python',\n",
       " 'togethercomputer/CodeLlama-13b',\n",
       " 'togethercomputer/CodeLlama-34b-Instruct',\n",
       " 'togethercomputer/CodeLlama-34b-Python',\n",
       " 'togethercomputer/CodeLlama-34b',\n",
       " 'togethercomputer/CodeLlama-7b-Instruct',\n",
       " 'togethercomputer/CodeLlama-7b-Python',\n",
       " 'togethercomputer/CodeLlama-7b',\n",
       " 'togethercomputer/GPT-JT-6B-v1',\n",
       " 'togethercomputer/GPT-JT-Moderation-6B',\n",
       " 'togethercomputer/GPT-NeoXT-Chat-Base-20B',\n",
       " 'togethercomputer/Koala-13B',\n",
       " 'togethercomputer/LLaMA-2-7B-32K',\n",
       " 'togethercomputer/Llama-2-7B-32K-Instruct',\n",
       " 'togethercomputer/Pythia-Chat-Base-7B-v0.16',\n",
       " 'togethercomputer/Qwen-7B-Chat',\n",
       " 'togethercomputer/Qwen-7B',\n",
       " 'togethercomputer/RedPajama-INCITE-7B-Base',\n",
       " 'togethercomputer/RedPajama-INCITE-7B-Chat',\n",
       " 'togethercomputer/RedPajama-INCITE-7B-Instruct',\n",
       " 'togethercomputer/RedPajama-INCITE-Base-3B-v1',\n",
       " 'togethercomputer/RedPajama-INCITE-Chat-3B-v1',\n",
       " 'togethercomputer/RedPajama-INCITE-Instruct-3B-v1',\n",
       " 'togethercomputer/alpaca-7b',\n",
       " 'togethercomputer/codegen2-16B',\n",
       " 'togethercomputer/codegen2-7B',\n",
       " 'togethercomputer/falcon-40b-instruct',\n",
       " 'togethercomputer/falcon-40b',\n",
       " 'togethercomputer/falcon-7b-instruct',\n",
       " 'togethercomputer/falcon-7b',\n",
       " 'togethercomputer/guanaco-13b',\n",
       " 'togethercomputer/guanaco-65b',\n",
       " 'togethercomputer/guanaco-7b',\n",
       " 'togethercomputer/llama-2-13b-chat',\n",
       " 'togethercomputer/llama-2-13b',\n",
       " 'togethercomputer/llama-2-70b-chat',\n",
       " 'togethercomputer/llama-2-70b',\n",
       " 'togethercomputer/llama-2-7b-chat',\n",
       " 'togethercomputer/llama-2-7b',\n",
       " 'togethercomputer/mpt-30b-instruct',\n",
       " 'togethercomputer/mpt-30b',\n",
       " 'togethercomputer/mpt-7b-chat',\n",
       " 'upstage/SOLAR-0-70b-16bit',\n",
       " 'wavymulder/Analog-Diffusion']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set your API key\n",
    "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "# list available models and descriptons\n",
    "models = together.Models.list()\n",
    "print(f\"{len(models)} models available\")\n",
    "\n",
    "# print the first 10 models on the menu\n",
    "model_names = [model_dict['name'] for model_dict in models]\n",
    "model_names[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314a65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#together.Models.start(\"togethercomputer/llama-2-70b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd7227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38518/994963610.py:17: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
    "    \"\"\"Together API key\"\"\"\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        extra = Extra.forbid\n",
    "\n",
    "#     @model_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the API key is set.\"\"\"\n",
    "        api_key = get_from_dict_or_env(\n",
    "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
    "        )\n",
    "        values[\"together_api_key\"] = api_key\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        together.api_key = self.together_api_key\n",
    "        output = together.Complete.create(prompt,\n",
    "                                          model=self.model,\n",
    "                                          max_tokens=self.max_tokens,\n",
    "                                          temperature=self.temperature,\n",
    "                                          )\n",
    "        text = output['output']['choices'][0]['text']\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaea45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('/home/austin/code/ai/RAGS/data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53acce1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b45df97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca0df10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='N.R., L.Z., and C.A.V . synthesized synthetic guides and advised on synthetic RNA experiments. K.H., J.A.W, A.P.K, and A.E.Z. \\nsynthesized synthetic guides and advised on synthetic RNA experiments. H.M., J.X., and G.G. produced AA V and adenovirus. S.K.D., \\nY .M., and D.R.R. provided primary human hepatocytes and advice for in vivo  experiments with humanized mouse models. L.F. and \\nG.B. provided humanized-liver mice, managed in vivo  injections and harvests, and advised on the in vivo  aspects of the project. \\nO.O.A. and J.S.G. wrote the manuscript with help from all authors. M.Y ., E.I.I., C.S., and R.N.K. contributed equally and have the \\nright to list their name first in their CV .\\nCode Availability:  Code to predict atgRNA efficiency and support information are available at https://github.com/abugoot-lab/\\natgRNA_rank .\\nCompeting interests:  O.O.A. and J.S.G. are co-inventors on patent applications filed by MIT relating to work in this manuscript.', metadata={'source': '/home/austin/code/ai/RAGS/data/PASTE.pdf', 'page': 0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f77b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "model_norm = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "#     model_kwargs={'device': 'cuda'},\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebcd19cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to run: 45.213992424993194\n"
     ]
    }
   ],
   "source": [
    "# create db (may need a gpu here)\n",
    "\n",
    "# %%time\n",
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "import time\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "persist_directory = 'db'\n",
    "\n",
    "# persist_directory = '/home/austin/code/ai/RAGS/db'\n",
    "## Here is the nmew embeddings being used\n",
    "embedding = model_norm\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=texts,\n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print(f'time taken to run embed ${len(texts)} chunks:',t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfae2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99cd4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4669dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default LLaMA-2 prompt style\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30db29fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST]<<SYS>>\\nYou are both a professor of medicine and a highly esteemed researcher in human genetic engineering. Your goal is to invent novel treatments for human cancers.\\n\\nAlways answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, provide what information is needed for the question to be answered. If you don't know the answer to a question, please don't share false information.\\n\\nYour superior logic and reasoning abilities coupled with you vast knowledge in biology, genetics, and medicine allow you to conduct innovative experiments resulting in significant advancements in medicine.\\n\\n<</SYS>>\\n\\nCONTEXT:/n/n {context}/n\\n\\nQuestion: {question}[/INST]\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_prompt = \"\"\"You are both a professor of medicine and a highly esteemed researcher in human genetic engineering. Your goal is to invent novel treatments for human cancers.\n",
    "\n",
    "Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, provide what information is needed for the question to be answered. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "Your superior logic and reasoning abilities coupled with you vast knowledge in biology, genetics, and medicine allow you to conduct innovative experiments resulting in significant advancements in medicine.\n",
    "\"\"\"\n",
    "\n",
    "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "get_prompt(instruction, sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3828b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = TogetherLLM(\n",
    "    model= \"togethercomputer/llama-2-70b-chat\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 2024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b732374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = get_prompt(instruction, sys_prompt)\n",
    "\n",
    "llama_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7694a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": llama_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62b9ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import prompt\n",
    "# create the chain to answer questions\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       chain_type_kwargs=chain_type_kwargs,\n",
    "                                       return_source_documents=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33b9e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb6acb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The PASTE experiment is about using a novel genome editing approach called PASTE (Programmable Adenovirus-\n",
      "mediated Somatic Genome Editing) to edit the human genome in vivo. The goal is to develop a treatment for\n",
      "human cancers by using PASTE to integrate specific genes into the human genome. The experiment involves\n",
      "delivering PASTE components to primary human hepatocytes and evaluating the integration efficiency and\n",
      "specificity of the approach.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/home/austin/code/ai/RAGS/data/PASTE.pdf\n",
      "/home/austin/code/ai/RAGS/data/PASTE.pdf\n",
      "/home/austin/code/ai/RAGS/data/PASTE.pdf\n",
      "/home/austin/code/ai/RAGS/data/PASTE.pdf\n",
      "/home/austin/code/ai/RAGS/data/PASTE.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the paste experiment about?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815e073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
